# 训练进度与配置日志 - 2026-01-21

## 1. 核心目标
在 `my_go2_jump` 框架下，实现 PD + 10Nm 初始阶段的稳定跳跃，并为后续 PD 衰减做好准备。

## 2. 修改记录
- **环境修复**: 重新锚定 `legged_gym` 包路径至 `my_go2_jump`，恢复 10 帧观测堆叠逻辑。
- **奖励调整**: 
    - `base_height` 权重: 5.0 -> **1.0** (防止过度追求高度导致翻车)。
    - `orientation` 权重: 2.0 -> 基础 **2.0** + 动态 Boost (随课程进度最高增至 5.0，强化姿态维持)。
- **控制对齐**: 确认 `decimation = 4` (50Hz)，确保力矩控制下的物理稳定性。

## 3. 训练表现 (Iteration 321)
- **Mean Reward**: ~110 (成功由负转正)。
- **Episode Length**: ~1150 (满分 1200)，表现出极高的生存能力。
- **关键指标**:
    - `rew_jump`: 0.74 (已初步掌握跳跃节奏)。
    - `rew_tracking_lin_vel`: 1.53 (速度跟踪良好)。
    - `rew_dof_acc`: -0.19 (运动极度平滑，无震荡)。
- **观察**: 机器人在预热期（PD 100% 开启）表现完美。

## 4. 下一步计划
- **监控过渡期**: 观察 Iteration 500 - 2000 阶段。随着 PD 增益线性衰减，观察 `orientation` 的动态增强是否能有效接管姿态维持。
- **性能评估**: 如果在 PD 消失后仍然稳定，则维持当前奖励权重；若出现失稳，进一步加大 `orientation` 或 `default_hip_pos` 的权重。

# 训练进度与配置日志 - 2026-01-22

## 1. 问题复盘：PD 衰减期的性能崩塌
- **现象**: 训练进入 Iter 500-2000 (PD 衰减期) 后，速度跟踪能力显著下降 (`rew_tracking_lin_vel` 从 1.9 跌至 1.3)，跳跃无力。
- **Play 验证异常**: 使用 `play.py` 测试 Checkpoint 800 时，表现有时异常好，有时速度跟不上。
- **根因分析**:
    1.  **观测缺失 (策略惰性)**: 策略网络无法感知 PD 刚度正在衰减 (Alpha 因子未加入 Obs)，导致它在需要接管控制时仍然输出微小的力矩（试图依赖已经变软的 PD）。
    2.  **验证失真**: `play.py` 默认将环境步数 (`common_step_counter`) 设为 0（PD 全开），导致在测试 Iter 800 模型时，其实是在用“半熟的脑子”控制“满血 PD 的身体”，造成测试结果与训练日志严重不符。

## 2. 核心架构升级 (Upgrade)
为了解决上述问题，进行了“最小改动但最关键”的升级：

- **A. 观测增强 (Observation Augmentation)**
    - **Obs 维度**: 59 -> **60**。
    - **内容**: 将 `curriculum_factor` (PD 衰减进度 Alpha) 显式加入 Observation 向量末尾。
    - **目的**: 让神经网络建立 `if PD_is_weak then output_large_torque` 的条件反射。
    - **配套**: 更新了 `obs_permutation` 以支持新维度。

- **B. 动态 Action Scale (Dynamic Action Scale)**
    - **逻辑**: `Action Scale` 不再固定为 10.0，而是随 PD 衰减反向增长。
    - **公式**: `Scale = 10.0 + Progress * (23.5 - 10.0)`。
    - **效果**: 在纯力矩阶段 (Progress=1.0)，最大 Scale 达到 23.5Nm，确保机器人有足够的爆发力进行跳跃。

- **C. 奖励微调**
    - `tracking_lin_vel` 权重: 2.0 -> **3.0** (强迫网络重视速度)。

## 3. 工具链修复
- **`play.py`**: 增加了自动同步逻辑。
    - 现在运行 `play.py --checkpoint=800` 会自动将环境的 `common_step_counter` 设为 19200，确保环境的 PD 刚度与模型训练阶段完全一致。
    - 修复了 Reward 打印逻辑，现在能持续打印每个 Episode 的真实奖励。

## 4. 验证与展望
- **Play 测试**: 在修复 `play.py` 后，Checkpoint 800 的 `rew_tracking_lin_vel` 达到了 **2.85** (远高于旧版的 1.3)。
- **结论**: 新架构下的模型在 PD 衰减期表现极其出色，成功克服了“软绵绵”的问题。
- **当前状态**: 已重新启动训练，预计在纯力矩阶段 (Iter 2000+) 将实现真正的残差力矩跳跃。